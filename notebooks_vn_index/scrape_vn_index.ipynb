{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"d27a16704c37129cb1ef782441d6b9e7\", element=\"f.F71EC71ADECEC04F50198EFDB1DCD609.d.669BA0645A3715B616FBEECC8E1EF7E4.e.481\")>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up Selenium WebDriver\n",
    "driver = webdriver.Chrome()  # Make sure chromedriver is in your PATH\n",
    "driver.get(\"http://en.stockbiz.vn/IndicesStats.aspx#\")\n",
    "\n",
    "# Wait for the page to load\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Wait for the calendar widget to be fully initialized\n",
    "WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.ID, \"ctl00_webPartManager_wp267165551_wp1192412521_dtStartDate_picker\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Date (Hidden): 2000.7.28\n",
      "Start Date (Visible):  7/28/2000\n",
      "End Date (Hidden): 2025.4.29\n",
      "End Date (Visible):  4/29/2025\n",
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Locate the hidden start date input field\n",
    "start_date_hidden_input = driver.find_element(By.ID, \"ctl00_webPartManager_wp267165551_wp1192412521_dtStartDate_picker_selecteddates\")\n",
    "\n",
    "# Locate the visible start date input field\n",
    "start_date_visible_input = driver.find_element(By.ID, \"ctl00_webPartManager_wp267165551_wp1192412521_dtStartDate_picker_picker\")\n",
    "\n",
    "# Set the start date value using JavaScript (both hidden and visible fields)\n",
    "start_date = \"28/07/2000\"  # Format: DD/MM/YYYY (adjust based on the website's expected format)\n",
    "driver.execute_script(f\"arguments[0].value = '{start_date}';\", start_date_hidden_input)\n",
    "driver.execute_script(f\"arguments[0].value = '{start_date}';\", start_date_visible_input)\n",
    "\n",
    "# Simulate the ComponentArt Calendar widget's internal logic\n",
    "# This JavaScript code sets the selected date in the calendar widget\n",
    "calendar_widget_script = \"\"\"\n",
    "var calendar = window.ctl00_webPartManager_wp267165551_wp1192412521_dtStartDate_picker;\n",
    "if (calendar && calendar.setSelectedDate) {\n",
    "    var selectedDate = new Date(2000, 6, 28);  // Year, Month (0-based), Day\n",
    "    calendar.setSelectedDate(selectedDate);\n",
    "    if (calendar.render) {\n",
    "        calendar.render();\n",
    "    } else {\n",
    "        console.error(\"render method not found on calendar widget.\");\n",
    "    }\n",
    "} else {\n",
    "    console.error(\"Calendar widget or setSelectedDate method not found.\");\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Execute the script to update the calendar widget\n",
    "driver.execute_script(calendar_widget_script)\n",
    "\n",
    "# Trigger events to ensure the website recognizes the change\n",
    "driver.execute_script(\"arguments[0].dispatchEvent(new Event('input'));\", start_date_hidden_input)\n",
    "driver.execute_script(\"arguments[0].dispatchEvent(new Event('change'));\", start_date_hidden_input)\n",
    "driver.execute_script(\"arguments[0].dispatchEvent(new Event('input'));\", start_date_visible_input)\n",
    "driver.execute_script(\"arguments[0].dispatchEvent(new Event('change'));\", start_date_visible_input)\n",
    "\n",
    "# Locate the hidden end date input field\n",
    "end_date_hidden_input = driver.find_element(By.ID, \"ctl00_webPartManager_wp267165551_wp1192412521_dtEndDate_picker_selecteddates\")\n",
    "\n",
    "# Locate the visible end date input field\n",
    "end_date_visible_input = driver.find_element(By.ID, \"ctl00_webPartManager_wp267165551_wp1192412521_dtEndDate_picker_picker\")\n",
    "\n",
    "# Set the end date value using JavaScript (both hidden and visible fields)\n",
    "end_date = datetime.today().strftime(\"%d/%m/%Y\")  # Format: DD/MM/YYYY (adjust based on the website's expected format)\n",
    "driver.execute_script(f\"arguments[0].value = '{end_date}';\", end_date_hidden_input)\n",
    "driver.execute_script(f\"arguments[0].value = '{end_date}';\", end_date_visible_input)\n",
    "\n",
    "# Simulate the ComponentArt Calendar widget's internal logic for the end date\n",
    "calendar_widget_script_end_date = \"\"\"\n",
    "var calendar = window.ctl00_webPartManager_wp267165551_wp1192412521_dtEndDate_picker;\n",
    "if (calendar && calendar.setSelectedDate) {\n",
    "    var selectedDate = new Date();  // Today's date\n",
    "    calendar.setSelectedDate(selectedDate);\n",
    "    if (calendar.render) {\n",
    "        calendar.render();\n",
    "    } else {\n",
    "        console.error(\"render method not found on calendar widget.\");\n",
    "    }\n",
    "} else {\n",
    "    console.error(\"Calendar widget or setSelectedDate method not found.\");\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Execute the script to update the calendar widget for the end date\n",
    "driver.execute_script(calendar_widget_script_end_date)\n",
    "\n",
    "# Trigger events to ensure the website recognizes the change\n",
    "driver.execute_script(\"arguments[0].dispatchEvent(new Event('input'));\", end_date_hidden_input)\n",
    "driver.execute_script(\"arguments[0].dispatchEvent(new Event('change'));\", end_date_hidden_input)\n",
    "driver.execute_script(\"arguments[0].dispatchEvent(new Event('input'));\", end_date_visible_input)\n",
    "driver.execute_script(\"arguments[0].dispatchEvent(new Event('change'));\", end_date_visible_input)\n",
    "\n",
    "# Debugging: Print the updated values\n",
    "start_date_hidden_value = driver.execute_script(\"return arguments[0].value;\", start_date_hidden_input)\n",
    "start_date_visible_value = driver.execute_script(\"return arguments[0].value;\", start_date_visible_input)\n",
    "end_date_hidden_value = driver.execute_script(\"return arguments[0].value;\", end_date_hidden_input)\n",
    "end_date_visible_value = driver.execute_script(\"return arguments[0].value;\", end_date_visible_input)\n",
    "print(\"Start Date (Hidden):\", start_date_hidden_value)\n",
    "print(\"Start Date (Visible):\", start_date_visible_value)\n",
    "print(\"End Date (Hidden):\", end_date_hidden_value)\n",
    "print(\"End Date (Visible):\", end_date_visible_value)\n",
    "\n",
    "# Locate and click the \"View\" button to refresh the data\n",
    "view_button = driver.find_element(By.ID, \"ctl00_webPartManager_wp267165551_wp1192412521_btnView\")\n",
    "driver.execute_script(\"arguments[0].click();\", view_button)  # Use JavaScript to click the button\n",
    "\n",
    "# Wait for the data to load\n",
    "try:\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"ctl00_webPartManager_wp267165551_wp1192412521_callbackData\"))\n",
    "    )\n",
    "    print(\"Data loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Data did not load:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape data from the current page\n",
    "def scrape_current_page(is_first_page):\n",
    "    # Locate the table or data container\n",
    "    table = driver.find_element(By.ID, \"ctl00_webPartManager_wp267165551_wp1192412521_callbackData\")\n",
    "\n",
    "    # Extract table rows\n",
    "    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "    # Extract data into a list\n",
    "    data = []\n",
    "    for i, row in enumerate(rows):\n",
    "        # Skip the footer row (last row)\n",
    "        if i == len(rows) - 1:\n",
    "            continue\n",
    "\n",
    "        # Skip the title row (first row) for pages 2 and beyond\n",
    "        if not is_first_page and i == 0:\n",
    "            continue\n",
    "\n",
    "        # Extract columns\n",
    "        cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "        cols = [col.text.strip() for col in cols[:-1]]\n",
    "        data.append(cols)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Function to get the total number of pages from the table footer\n",
    "def get_total_pages():\n",
    "    # Locate the table footer\n",
    "    table_footer = driver.find_element(By.CLASS_NAME, \"TableFooter\")\n",
    "\n",
    "    # Extract the text (e.g., \"Page 1 of 299 Next>\")\n",
    "    footer_text = table_footer.text\n",
    "\n",
    "    # Extract the total number of pages\n",
    "    # Example: \"Page 1 of 299 Next>\" -> Extract \"299\"\n",
    "    total_pages_text = footer_text.split(\"of\")[1].strip()  # \"299 Next>\"\n",
    "    total_pages = int(total_pages_text.split()[0])  # Extract \"299\" and convert to int\n",
    "    return total_pages\n",
    "\n",
    "def remove_outliers(df):\n",
    "    # Extract percentage from Change column and convert to decimal\n",
    "    df['Change'] = df['Change'].str.extract(r\"\\(([-+]?[\\d.]+)%\\)\").astype(float) / 100\n",
    "\n",
    "    # keep only rows where |Change| ≤ 0.03\n",
    "    df = df[df['Change'].abs() <= 0.03]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages to scrape: 301\n",
      "Scraped data from page 2.\n",
      "Scraped data from page 3.\n",
      "Scraped data from page 4.\n",
      "Scraped data from page 5.\n",
      "Scraped data from page 6.\n",
      "Scraped data from page 7.\n",
      "Scraped data from page 8.\n",
      "Scraped data from page 9.\n",
      "Scraped data from page 10.\n",
      "Scraped data from page 11.\n",
      "Scraped data from page 12.\n",
      "Scraped data from page 13.\n",
      "Scraped data from page 14.\n",
      "Scraped data from page 15.\n",
      "Scraped data from page 16.\n",
      "Scraped data from page 17.\n",
      "Scraped data from page 18.\n",
      "Scraped data from page 19.\n",
      "Scraped data from page 20.\n",
      "Scraped data from page 21.\n",
      "Scraped data from page 22.\n",
      "Scraped data from page 23.\n",
      "Scraped data from page 24.\n",
      "Scraped data from page 25.\n",
      "Scraped data from page 26.\n",
      "Scraped data from page 27.\n",
      "Scraped data from page 28.\n",
      "Scraped data from page 29.\n",
      "Scraped data from page 30.\n",
      "Scraped data from page 31.\n",
      "Scraped data from page 32.\n",
      "Scraped data from page 33.\n",
      "Scraped data from page 34.\n",
      "Scraped data from page 35.\n",
      "Scraped data from page 36.\n",
      "Scraped data from page 37.\n",
      "Scraped data from page 38.\n",
      "Scraped data from page 39.\n",
      "Scraped data from page 40.\n",
      "Scraped data from page 41.\n",
      "Scraped data from page 42.\n",
      "Scraped data from page 43.\n",
      "Scraped data from page 44.\n",
      "Scraped data from page 45.\n",
      "Scraped data from page 46.\n",
      "Scraped data from page 47.\n",
      "Scraped data from page 48.\n",
      "Scraped data from page 49.\n",
      "Scraped data from page 50.\n",
      "Scraped data from page 51.\n",
      "Scraped data from page 52.\n",
      "Scraped data from page 53.\n",
      "Scraped data from page 54.\n",
      "Scraped data from page 55.\n",
      "Scraped data from page 56.\n",
      "Scraped data from page 57.\n",
      "Scraped data from page 58.\n",
      "Scraped data from page 59.\n",
      "Scraped data from page 60.\n",
      "Scraped data from page 61.\n",
      "Scraped data from page 62.\n",
      "Scraped data from page 63.\n",
      "Scraped data from page 64.\n",
      "Scraped data from page 65.\n",
      "Scraped data from page 66.\n",
      "Scraped data from page 67.\n",
      "Scraped data from page 68.\n",
      "Scraped data from page 69.\n",
      "Scraped data from page 70.\n",
      "Scraped data from page 71.\n",
      "Scraped data from page 72.\n",
      "Scraped data from page 73.\n",
      "Scraped data from page 74.\n",
      "Scraped data from page 75.\n",
      "Scraped data from page 76.\n",
      "Scraped data from page 77.\n",
      "Scraped data from page 78.\n",
      "Scraped data from page 79.\n",
      "Scraped data from page 80.\n",
      "Scraped data from page 81.\n",
      "Scraped data from page 82.\n",
      "Scraped data from page 83.\n",
      "Scraped data from page 84.\n",
      "Scraped data from page 85.\n",
      "Scraped data from page 86.\n",
      "Scraped data from page 87.\n",
      "Scraped data from page 88.\n",
      "Scraped data from page 89.\n",
      "Scraped data from page 90.\n",
      "Scraped data from page 91.\n",
      "Scraped data from page 92.\n",
      "Scraped data from page 93.\n",
      "Scraped data from page 94.\n",
      "Scraped data from page 95.\n",
      "Scraped data from page 96.\n",
      "Scraped data from page 97.\n",
      "Scraped data from page 98.\n",
      "Scraped data from page 99.\n",
      "Scraped data from page 100.\n",
      "Scraped data from page 101.\n",
      "Scraped data from page 102.\n",
      "Scraped data from page 103.\n",
      "Scraped data from page 104.\n",
      "Scraped data from page 105.\n",
      "Scraped data from page 106.\n",
      "Scraped data from page 107.\n",
      "Scraped data from page 108.\n",
      "Scraped data from page 109.\n",
      "Scraped data from page 110.\n",
      "Scraped data from page 111.\n",
      "Scraped data from page 112.\n",
      "Scraped data from page 113.\n",
      "Scraped data from page 114.\n",
      "Scraped data from page 115.\n",
      "Scraped data from page 116.\n",
      "Scraped data from page 117.\n",
      "Scraped data from page 118.\n",
      "Scraped data from page 119.\n",
      "Scraped data from page 120.\n",
      "Scraped data from page 121.\n",
      "Scraped data from page 122.\n",
      "Scraped data from page 123.\n",
      "Scraped data from page 124.\n",
      "Scraped data from page 125.\n",
      "Scraped data from page 126.\n",
      "Scraped data from page 127.\n",
      "Scraped data from page 128.\n",
      "Scraped data from page 129.\n",
      "Scraped data from page 130.\n",
      "Scraped data from page 131.\n",
      "Scraped data from page 132.\n",
      "Scraped data from page 133.\n",
      "Scraped data from page 134.\n",
      "Scraped data from page 135.\n",
      "Scraped data from page 136.\n",
      "Scraped data from page 137.\n",
      "Scraped data from page 138.\n",
      "Scraped data from page 139.\n",
      "Scraped data from page 140.\n",
      "Scraped data from page 141.\n",
      "Scraped data from page 142.\n",
      "Scraped data from page 143.\n",
      "Scraped data from page 144.\n",
      "Scraped data from page 145.\n",
      "Scraped data from page 146.\n",
      "Scraped data from page 147.\n",
      "Scraped data from page 148.\n",
      "Scraped data from page 149.\n",
      "Scraped data from page 150.\n",
      "Scraped data from page 151.\n",
      "Scraped data from page 152.\n",
      "Scraped data from page 153.\n",
      "Scraped data from page 154.\n",
      "Scraped data from page 155.\n",
      "Scraped data from page 156.\n",
      "Scraped data from page 157.\n",
      "Scraped data from page 158.\n",
      "Scraped data from page 159.\n",
      "Scraped data from page 160.\n",
      "Scraped data from page 161.\n",
      "Scraped data from page 162.\n",
      "Scraped data from page 163.\n",
      "Scraped data from page 164.\n",
      "Scraped data from page 165.\n",
      "Scraped data from page 166.\n",
      "Scraped data from page 167.\n",
      "Scraped data from page 168.\n",
      "Scraped data from page 169.\n",
      "Scraped data from page 170.\n",
      "Scraped data from page 171.\n",
      "Scraped data from page 172.\n",
      "Scraped data from page 173.\n",
      "Scraped data from page 174.\n",
      "Scraped data from page 175.\n",
      "Scraped data from page 176.\n",
      "Scraped data from page 177.\n",
      "Scraped data from page 178.\n",
      "Scraped data from page 179.\n",
      "Scraped data from page 180.\n",
      "Scraped data from page 181.\n",
      "Scraped data from page 182.\n",
      "Scraped data from page 183.\n",
      "Scraped data from page 184.\n",
      "Scraped data from page 185.\n",
      "Scraped data from page 186.\n",
      "Scraped data from page 187.\n",
      "Scraped data from page 188.\n",
      "Scraped data from page 189.\n",
      "Scraped data from page 190.\n",
      "Scraped data from page 191.\n",
      "Scraped data from page 192.\n",
      "Scraped data from page 193.\n",
      "Scraped data from page 194.\n",
      "Scraped data from page 195.\n",
      "Scraped data from page 196.\n",
      "Scraped data from page 197.\n",
      "Scraped data from page 198.\n",
      "Scraped data from page 199.\n",
      "Scraped data from page 200.\n",
      "Scraped data from page 201.\n",
      "Scraped data from page 202.\n",
      "Scraped data from page 203.\n",
      "Scraped data from page 204.\n",
      "Scraped data from page 205.\n",
      "Scraped data from page 206.\n",
      "Scraped data from page 207.\n",
      "Scraped data from page 208.\n",
      "Scraped data from page 209.\n",
      "Scraped data from page 210.\n",
      "Scraped data from page 211.\n",
      "Scraped data from page 212.\n",
      "Scraped data from page 213.\n",
      "Scraped data from page 214.\n",
      "Scraped data from page 215.\n",
      "Scraped data from page 216.\n",
      "Scraped data from page 217.\n",
      "Scraped data from page 218.\n",
      "Scraped data from page 219.\n",
      "Scraped data from page 220.\n",
      "Scraped data from page 221.\n",
      "Scraped data from page 222.\n",
      "Scraped data from page 223.\n",
      "Scraped data from page 224.\n",
      "Scraped data from page 225.\n",
      "Scraped data from page 226.\n",
      "Scraped data from page 227.\n",
      "Scraped data from page 228.\n",
      "Scraped data from page 229.\n",
      "Scraped data from page 230.\n",
      "Scraped data from page 231.\n",
      "Scraped data from page 232.\n",
      "Scraped data from page 233.\n",
      "Scraped data from page 234.\n",
      "Scraped data from page 235.\n",
      "Scraped data from page 236.\n",
      "Scraped data from page 237.\n",
      "Scraped data from page 238.\n",
      "Scraped data from page 239.\n",
      "Scraped data from page 240.\n",
      "Scraped data from page 241.\n",
      "Scraped data from page 242.\n",
      "Scraped data from page 243.\n",
      "Scraped data from page 244.\n",
      "Scraped data from page 245.\n",
      "Scraped data from page 246.\n",
      "Scraped data from page 247.\n",
      "Scraped data from page 248.\n",
      "Scraped data from page 249.\n",
      "Scraped data from page 250.\n",
      "Scraped data from page 251.\n",
      "Scraped data from page 252.\n",
      "Scraped data from page 253.\n",
      "Scraped data from page 254.\n",
      "Scraped data from page 255.\n",
      "Scraped data from page 256.\n",
      "Scraped data from page 257.\n",
      "Scraped data from page 258.\n",
      "Scraped data from page 259.\n",
      "Scraped data from page 260.\n",
      "Scraped data from page 261.\n",
      "Scraped data from page 262.\n",
      "Scraped data from page 263.\n",
      "Scraped data from page 264.\n",
      "Scraped data from page 265.\n",
      "Scraped data from page 266.\n",
      "Scraped data from page 267.\n",
      "Scraped data from page 268.\n",
      "Scraped data from page 269.\n",
      "Scraped data from page 270.\n",
      "Scraped data from page 271.\n",
      "Scraped data from page 272.\n",
      "Scraped data from page 273.\n",
      "Scraped data from page 274.\n",
      "Scraped data from page 275.\n",
      "Scraped data from page 276.\n",
      "Scraped data from page 277.\n",
      "Scraped data from page 278.\n",
      "Scraped data from page 279.\n",
      "Scraped data from page 280.\n",
      "Scraped data from page 281.\n",
      "Scraped data from page 282.\n",
      "Scraped data from page 283.\n",
      "Scraped data from page 284.\n",
      "Scraped data from page 285.\n",
      "Scraped data from page 286.\n",
      "Scraped data from page 287.\n",
      "Scraped data from page 288.\n",
      "Scraped data from page 289.\n",
      "Scraped data from page 290.\n",
      "Scraped data from page 291.\n",
      "Scraped data from page 292.\n",
      "Scraped data from page 293.\n",
      "Scraped data from page 294.\n",
      "Scraped data from page 295.\n",
      "Scraped data from page 296.\n",
      "Scraped data from page 297.\n",
      "Scraped data from page 298.\n",
      "Scraped data from page 299.\n",
      "Scraped data from page 300.\n",
      "Scraped data from page 301.\n",
      "Data saved to hose_historical_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store all data\n",
    "all_data = []\n",
    "\n",
    "# Scrape data from the first page (including the title row)\n",
    "first_page_data = scrape_current_page(is_first_page=True)\n",
    "title_row = first_page_data[0]  # Extract the title row\n",
    "all_data.extend(first_page_data[1:])  # Append the rest of the data (excluding the title row)\n",
    "\n",
    "# Get the total number of pages from the table footer\n",
    "total_pages = get_total_pages()\n",
    "print(f\"Total pages to scrape: {total_pages}\")\n",
    "\n",
    "# Initialize the offset for pagination\n",
    "offset = 20\n",
    "\n",
    "# Loop through all pages\n",
    "for page in range(2, total_pages + 1):  # Start from page 2 (since we already scraped page 1)\n",
    "    try:\n",
    "        # Locate the \"Next\" button\n",
    "        next_button = driver.find_element(By.XPATH, \"//a[contains(@onclick, 'RefreshData')]\")\n",
    "\n",
    "        # Update the \"Next\" button's onclick attribute to the correct offset\n",
    "        driver.execute_script(f\"arguments[0].setAttribute('onclick', 'RefreshData({offset})');\", next_button)\n",
    "\n",
    "        # Click the \"Next\" button using JavaScript\n",
    "        driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "\n",
    "        # Wait for the data to load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"ctl00_webPartManager_wp267165551_wp1192412521_callbackData\"))\n",
    "        )\n",
    "\n",
    "        # Scrape data from the current page (excluding the title row)\n",
    "        all_data.extend(scrape_current_page(is_first_page=False))\n",
    "        print(f\"Scraped data from page {page}.\")\n",
    "\n",
    "        # Increment the offset by 20 for the next page\n",
    "        offset += 20\n",
    "\n",
    "    except Exception as e:\n",
    "        # If the \"Next\" button is not found or an error occurs, stop the loop\n",
    "        print(\"No more pages or an error occurred:\", e)\n",
    "        break\n",
    "\n",
    "# Convert the data into a pandas DataFrame\n",
    "df = pd.DataFrame(all_data, columns=title_row)  # Use the title row as column headers\n",
    "\n",
    "# # Add an index column\n",
    "# df.reset_index(inplace=True)  # Add a sequential index column\n",
    "# df.rename(columns={\"index\": \"Index\"}, inplace=True)  # Rename the index column to \"Index\"\n",
    "\n",
    "df = remove_outliers(df)\n",
    "\n",
    "# Save the data to a CSV file\n",
    "df.to_csv(\"../vn_index/hose_historical_data.csv\", index=False)  # Do not use any column as the index\n",
    "print(\"Data saved to hose_historical_data.csv\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read existing CSV\n",
    "# df = pd.read_csv(\"../vn_index/hose_historical_data.csv\")\n",
    "\n",
    "# # Scrape the first page\n",
    "# first_page_data = scrape_current_page(is_first_page=True)\n",
    "\n",
    "# # Extract header and data\n",
    "# title_row = first_page_data[0]\n",
    "# first_page_df = pd.DataFrame(first_page_data[2:], columns=title_row)\n",
    "# first_page_df = remove_outliers(first_page_df)\n",
    "\n",
    "# # Combine the new and existing data\n",
    "# combined_df = pd.concat([first_page_df, df], ignore_index=True)\n",
    "\n",
    "# # Drop duplicates based on 'Date' column only — keep the first occurrence\n",
    "# combined_df.drop_duplicates(subset=['Date'], keep='first', inplace=True)\n",
    "\n",
    "# # Drop old 'Index' column if it's already there\n",
    "# if 'Index' in combined_df.columns:\n",
    "#     combined_df.drop(columns=['Index'], inplace=True)\n",
    "\n",
    "# # Reset index and insert 'Index' column as a normal column\n",
    "# combined_df.reset_index(drop=True, inplace=True)\n",
    "# # combined_df.insert(0, 'Index', combined_df.index)\n",
    "\n",
    "# # Save to CSV\n",
    "# combined_df.to_csv(\"../vn_index/hose_historical_data.csv\", index=False)\n",
    "# print(\"Data saved to hose_historical_data.csv\")\n",
    "\n",
    "# # Quit driver\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_page_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df[:60]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
