{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import optuna\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL AND TRAINING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM Model for time-series forecasting.\n",
    "    \n",
    "    Attributes:\n",
    "        lstm (nn.LSTM): LSTM layer.\n",
    "        fc (nn.Linear): Fully connected output layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM model.\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): Number of input features.\n",
    "            hidden_size (int): Number of hidden units.\n",
    "            num_layers (int): Number of LSTM layers.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n",
    "                            batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor from the last time step.\n",
    "        \"\"\"\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        return self.fc(lstm_out[:, -1, :])  # Use output from the last time step\n",
    "\n",
    "\n",
    "def create_sequences(data, dates, seq_len=30):\n",
    "    \"\"\"\n",
    "    Create sequences for LSTM input.\n",
    "    \n",
    "    Args:\n",
    "        data (np.array): Scaled data as a NumPy array.\n",
    "        dates (array-like): Corresponding dates.\n",
    "        seq_len (int): Sequence length.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Arrays for features (X), targets (y), and corresponding dates.\n",
    "    \"\"\"\n",
    "    X, y, y_dates = [], [], []\n",
    "    for i in range(len(data) - seq_len):\n",
    "        X.append(data[i:i+seq_len])\n",
    "        y.append(data[i+seq_len, 0])\n",
    "        y_dates.append(dates[i+seq_len])\n",
    "    return np.array(X), np.array(y), np.array(y_dates)\n",
    "\n",
    "\n",
    "def inverse_scale_predictions(predictions, scaler):\n",
    "    \"\"\"\n",
    "    Inverse scale predictions using the fitted scaler.\n",
    "    \n",
    "    Args:\n",
    "        predictions (np.array): Scaled predictions.\n",
    "        scaler (MinMaxScaler): Fitted scaler.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Inverse-transformed predictions.\n",
    "    \"\"\"\n",
    "    num_features = scaler.min_.shape[0]\n",
    "    dummy = np.zeros((predictions.shape[0], num_features))\n",
    "    dummy[:, 0] = predictions.flatten()\n",
    "    return scaler.inverse_transform(dummy)[:, 0]\n",
    "\n",
    "\n",
    "def train_model(params, X_train_tensor, y_train_tensor, epochs=50, early_stop_patience=10, trial=None):\n",
    "    \"\"\"\n",
    "    Train the LSTM model on the training data with early stopping.\n",
    "    \n",
    "    Args:\n",
    "        params (dict): Hyperparameters for training.\n",
    "        X_train_tensor (torch.Tensor): Training features.\n",
    "        y_train_tensor (torch.Tensor): Training targets.\n",
    "        epochs (int): Maximum number of training epochs.\n",
    "        early_stop_patience (int): Number of epochs with no improvement to trigger early stopping.\n",
    "        trial (optuna.trial.Trial, optional): If provided, report intermediate results for pruning.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model, train_loss_list)\n",
    "               model: The trained LSTM model (with the best observed weights).\n",
    "               train_loss_list: List of training losses per epoch.\n",
    "    \"\"\"\n",
    "    model = LSTMModel(\n",
    "        input_size=X_train_tensor.shape[2],\n",
    "        hidden_size=params['hidden_size'],\n",
    "        num_layers=params['num_layers'],\n",
    "        dropout=params['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=1e-5)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "    \n",
    "    train_loss_list = []\n",
    "    best_loss = np.inf\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        for batch_X, batch_y in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{epochs}\", leave=False):\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(batch_X), batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "        train_loss_list.append(avg_train_loss)\n",
    "        \n",
    "        # Report intermediate loss to Optuna (if trial is provided)\n",
    "        if trial is not None:\n",
    "            trial.report(avg_train_loss, epoch)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_train_loss < best_loss:\n",
    "            best_loss = avg_train_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= early_stop_patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model state if early stopping occurred\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, train_loss_list\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_eval_tensor, y_eval_tensor, batch_size):\n",
    "    \"\"\"\n",
    "    Evaluate the LSTM model on an evaluation (validation or test) set.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Trained LSTM model.\n",
    "        X_eval_tensor (torch.Tensor): Evaluation features.\n",
    "        y_eval_tensor (torch.Tensor): Evaluation targets.\n",
    "        batch_size (int): Batch size for evaluation.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (avg_eval_loss, y_pred_tensor)\n",
    "               avg_eval_loss: Average loss over the evaluation set.\n",
    "               y_pred_tensor: Model predictions on the evaluation set.\n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    eval_dataset = TensorDataset(X_eval_tensor, y_eval_tensor)\n",
    "    eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in eval_loader:\n",
    "            eval_loss += criterion(model(batch_X), batch_y).item()\n",
    "    avg_eval_loss = eval_loss / len(eval_loader)\n",
    "    \n",
    "    # Obtain predictions on the evaluation set\n",
    "    y_pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, _ in eval_loader:\n",
    "            y_pred_list.append(model(batch_X))\n",
    "    y_pred_tensor = torch.cat(y_pred_list, dim=0).cpu().numpy()\n",
    "    \n",
    "    return avg_eval_loss, y_pred_tensor\n",
    "\n",
    "\n",
    "def objective(trial, X_train_tensor, y_train_tensor):\n",
    "    \"\"\"\n",
    "    Standard Optuna objective function for hyperparameter tuning.\n",
    "    This function only trains the model on the training set (with early stopping) and returns the final training loss.\n",
    "    \n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): Optuna trial object.\n",
    "        X_train_tensor (torch.Tensor): Training features.\n",
    "        y_train_tensor (torch.Tensor): Training targets.\n",
    "    \n",
    "    Returns:\n",
    "        float: Final training loss (last epoch) for the given hyperparameters.\n",
    "    \"\"\"\n",
    "    clear_memory()\n",
    "\n",
    "    params = {\n",
    "        'hidden_size': trial.suggest_categorical('hidden_size', [64, 128, 256]),\n",
    "        'num_layers': trial.suggest_int('num_layers', 1, 3),\n",
    "        'dropout': trial.suggest_float('dropout', 0.1, 0.5),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [32, 64])\n",
    "    }\n",
    "    \n",
    "    # Train only on the training set with early stopping and report intermediate results for pruning\n",
    "    _, train_loss_list = train_model(params, X_train_tensor, y_train_tensor, epochs=20, early_stop_patience=10, trial=trial)\n",
    "    trial.set_user_attr(\"train_loss_list\", train_loss_list)\n",
    "    \n",
    "    clear_memory()\n",
    "    \n",
    "    # Return the final training loss as the objective\n",
    "    return train_loss_list[-1]\n",
    "\n",
    "\n",
    "def lstm_model_pipeline(data, seq_len=30, tuning=True, best_params=None, early_stop_patience=10):\n",
    "    \"\"\"\n",
    "    Pipeline for training, tuning, evaluation, and prediction using the LSTM model.\n",
    "    After tuning, the pipeline prints the best hyperparameters, plots:\n",
    "      - Training loss curve\n",
    "      - Predicted vs. actual VN-INDEX\n",
    "      - Residual histogram\n",
    "    and computes evaluation metrics (RMSE, MAE, R2, and Directional Accuracy).\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame or pd.Series): Original dataset.\n",
    "        seq_len (int): Length of input sequences.\n",
    "        tuning (bool): Whether to perform hyperparameter tuning.\n",
    "        best_params (dict): Default best parameters if tuning is not performed.\n",
    "        early_stop_patience (int): Number of epochs with no improvement to trigger early stopping.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (best_params, train_loss_list, val_loss, y_pred, scaler, y_dates_test)\n",
    "               which can be used for further evaluation.\n",
    "    \"\"\"\n",
    "    if best_params is None:\n",
    "        best_params = {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.3, \n",
    "                       'learning_rate': 1e-4, 'batch_size': 32}\n",
    "    \n",
    "    # Convert Series to DataFrame if needed\n",
    "    if isinstance(data, pd.Series):\n",
    "        data = data.to_frame()\n",
    "    \n",
    "    # Normalize data: fit scaler on first 90% of data, then transform all\n",
    "    train_size = int(0.9 * len(data))\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(data.iloc[:train_size])\n",
    "    data_scaled = scaler.transform(data)\n",
    "    \n",
    "    # Create sequences and split into train/test based on original indices\n",
    "    X, y, y_dates = create_sequences(data_scaled, data.index, seq_len)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "    y_dates_train, y_dates_test = y_dates[:train_size], y_dates[train_size:]\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "    \n",
    "    if tuning:\n",
    "        print(\"Starting hyperparameter tuning with Optuna...\")\n",
    "        # Use a pruner to cut unpromising trials early\n",
    "        pruner = optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    "        study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "        # Reduce number of trials to ease computational load (e.g., n_trials=10)\n",
    "        study.optimize(\n",
    "            lambda trial: objective(trial, X_train_tensor, y_train_tensor),\n",
    "            n_trials=10\n",
    "        )\n",
    "        best_trial = study.best_trial\n",
    "        best_params = best_trial.params\n",
    "        train_loss_list = best_trial.user_attrs[\"train_loss_list\"]\n",
    "        print(\"Best Hyperparameters:\", best_params)\n",
    "        print(\"Final Training Loss during tuning:\", train_loss_list[-1])\n",
    "        \n",
    "        # After tuning, retrain on the training set using the best hyperparameters\n",
    "        model, train_loss_list = train_model(best_params, X_train_tensor, y_train_tensor, epochs=20, early_stop_patience=early_stop_patience)\n",
    "        # Evaluate on the test set\n",
    "        val_loss, y_pred_tensor = evaluate_model(model, X_test_tensor, y_test_tensor, best_params['batch_size'])\n",
    "        y_pred = inverse_scale_predictions(y_pred_tensor, scaler)\n",
    "        print(\"Final Evaluation Loss on Test Set:\", val_loss)\n",
    "    else:\n",
    "        # Train using the provided hyperparameters\n",
    "        model, train_loss_list = train_model(best_params, X_train_tensor, y_train_tensor, epochs=50, early_stop_patience=early_stop_patience)\n",
    "        print(\"Using provided Hyperparameters:\", best_params)\n",
    "        print(\"Final Training Loss:\", train_loss_list[-1])\n",
    "        # Evaluate on the test set\n",
    "        val_loss, y_pred_tensor = evaluate_model(model, X_test_tensor, y_test_tensor, best_params['batch_size'])\n",
    "        y_pred = inverse_scale_predictions(y_pred_tensor, scaler)\n",
    "        print(\"Final Evaluation Loss on Test Set:\", val_loss)\n",
    "\n",
    "    # --------------------\n",
    "    # Compute Evaluation Metrics\n",
    "    # --------------------\n",
    "    y_true = inverse_scale_predictions(y_test_tensor.cpu().numpy(), scaler)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    # Compute directional accuracy: percentage of times the sign of the change is predicted correctly.\n",
    "    if len(y_true) > 1:\n",
    "        directional_accuracy = np.mean(np.sign(np.diff(y_true)) == np.sign(np.diff(y_pred)))\n",
    "    else:\n",
    "        directional_accuracy = np.nan\n",
    "        \n",
    "    print(\"RMSE:\", rmse)\n",
    "    print(\"MAE:\", mae)\n",
    "    print(\"R2 Score:\", r2)\n",
    "    print(\"Directional Accuracy:\", directional_accuracy)\n",
    "    # --------------------\n",
    "    # Plot Predicted vs. Actual VN-INDEX\n",
    "    # --------------------\n",
    "    y_true = inverse_scale_predictions(y_test_tensor.cpu().numpy(), scaler)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_dates_test, y_true, label=\"Actual VN-INDEX\", marker='o', color=\"blue\")\n",
    "    plt.plot(y_dates_test, y_pred, label=\"Predicted VN-INDEX\", marker='s', linestyle=\"dashed\", color=\"red\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"VN-INDEX\")\n",
    "    plt.title(\"Predicted vs. Actual VN-INDEX\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    plt.show()\n",
    "    \n",
    "    # --------------------\n",
    "    # Plot Residual Histogram\n",
    "    # --------------------\n",
    "    residuals = y_true - y_pred\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.hist(residuals, bins=30, edgecolor='k', alpha=0.7)\n",
    "    plt.title(\"Residual Histogram\")\n",
    "    plt.xlabel(\"Residual\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    plt.show()\n",
    "    \n",
    "    return model, X_test_tensor, scaler, y_pred\n",
    "\n",
    "\n",
    "def future_prediction(X_test, y_pred, data, scaler, model, num_days=30):\n",
    "    \"\"\"\n",
    "    Generate future predictions based on the last available test sequence.\n",
    "    \n",
    "    Args:\n",
    "        X_test (torch.Tensor): Test set features tensor.\n",
    "        y_pred (array): Predicted VN-INDEX values on the test set.\n",
    "        data (pd.DataFrame): Original dataset.\n",
    "        scaler (MinMaxScaler): Fitted scaler.\n",
    "        model (nn.Module): Trained LSTM model.\n",
    "        num_days (int): Number of future days to predict.\n",
    "    \"\"\"\n",
    "    if isinstance(data, pd.Series):\n",
    "        data = data.to_frame()\n",
    "    \n",
    "    model.eval()\n",
    "    input_seq = X_test[-1].cpu().numpy()\n",
    "    future_preds = []\n",
    "    for _ in range(num_days):\n",
    "        input_tensor = torch.tensor(input_seq, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(input_tensor).cpu().numpy()[0, 0]\n",
    "        future_preds.append(pred)\n",
    "        input_seq = np.roll(input_seq, -1, axis=0)\n",
    "        input_seq[-1, 0] = pred  # update the VN-INDEX feature\n",
    "\n",
    "    future_preds = inverse_scale_predictions(np.array(future_preds).reshape(-1,1), scaler)\n",
    "    \n",
    "    # Generate future trading dates (skipping weekends)\n",
    "    last_date = data.index[-1]\n",
    "    future_dates = []\n",
    "    while len(future_dates) < num_days:\n",
    "        last_date += pd.Timedelta(days=1)\n",
    "        if last_date.weekday() < 5:\n",
    "            future_dates.append(last_date)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(future_dates, future_preds, marker='o', linestyle=\"dashed\", color=\"red\", label=\"Predicted VN-INDEX\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"VN-INDEX\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    plt.title(f\"Predicted VN-INDEX for Next {num_days} Trading Days\")\n",
    "    plt.show()\n",
    "    \n",
    "    historical_dates = data.index[-100:]\n",
    "    historical_values = data.iloc[-100:, 0].values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(historical_dates, historical_values, label=\"Historical VN-INDEX\", color=\"blue\")\n",
    "    plt.plot(historical_dates, y_pred[-100:], label=\"Test Predictions\", color=\"red\")\n",
    "    plt.plot(future_dates, future_preds, color=\"green\", label=\"Future Predicted VN-INDEX\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"VN-INDEX\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    plt.title(\"Historical VN-INDEX with Future Predictions\")\n",
    "    plt.show()\n",
    "    \n",
    "    future_df = pd.DataFrame({\"Date\": future_dates, \"Predicted VN-INDEX\": future_preds})\n",
    "    print(\"Future Predictions:\")\n",
    "    print(future_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE ENGINEERING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def compute_RSI(series, window=14):\n",
    "    \"\"\"\n",
    "    Compute the Relative Strength Index (RSI) for a time-series.\n",
    "    \n",
    "    Args:\n",
    "        series (pd.Series): Series of prices.\n",
    "        window (int): Window size.\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: RSI values.\n",
    "    \"\"\"\n",
    "    delta = series.diff(1)\n",
    "    gain = delta.where(delta > 0, 0).rolling(window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window).mean()\n",
    "    RS = gain / loss\n",
    "    return 100 - (100 / (1 + RS))\n",
    "\n",
    "\n",
    "def lag_features_indicators(df, numerical_columns):\n",
    "    \"\"\"\n",
    "    Generate lag features, moving averages, RSI, MACD, volatility, seasonality,\n",
    "    and interaction features.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input data.\n",
    "        numerical_columns (list): List of numerical column names.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with additional features.\n",
    "    \"\"\"\n",
    "    copy_df = df.copy()\n",
    "    # Lagged Features\n",
    "    lag_days = [1, 2, 3, 5, 10]\n",
    "    for col in numerical_columns:\n",
    "        for lag in lag_days:\n",
    "            copy_df[f'{col}_Lag{lag}'] = copy_df[col].shift(lag)\n",
    "    \n",
    "    # Moving Averages and Exponential Moving Averages\n",
    "    for col in numerical_columns:\n",
    "        copy_df[f'{col}_SMA_10'] = copy_df[col].rolling(window=10).mean()\n",
    "        copy_df[f'{col}_SMA_20'] = copy_df[col].rolling(window=20).mean()\n",
    "        copy_df[f'{col}_EMA_10'] = copy_df[col].ewm(span=10, adjust=False).mean()\n",
    "        copy_df[f'{col}_EMA_20'] = copy_df[col].ewm(span=20, adjust=False).mean()\n",
    "    \n",
    "    # Relative Strength Index (RSI)\n",
    "    for col in numerical_columns:\n",
    "        copy_df[f'{col}_RSI_14'] = compute_RSI(copy_df[col])\n",
    "    \n",
    "    # Moving Average Convergence Divergence (MACD)\n",
    "    for col in numerical_columns:\n",
    "        copy_df[f'{col}_EMA_12'] = copy_df[col].ewm(span=12, adjust=False).mean()\n",
    "        copy_df[f'{col}_EMA_26'] = copy_df[col].ewm(span=26, adjust=False).mean()\n",
    "        copy_df[f'{col}_MACD'] = copy_df[f'{col}_EMA_12'] - copy_df[f'{col}_EMA_26']\n",
    "    \n",
    "    # Additional Feature: Rolling Standard Deviation for Volatility\n",
    "    for col in numerical_columns:\n",
    "        copy_df[f'{col}_RollingStd_10'] = copy_df[col].rolling(window=10).std()\n",
    "    \n",
    "    # Seasonality Features: Day of Week and Month\n",
    "    copy_df['DayOfWeek'] = copy_df.index.dayofweek\n",
    "    copy_df['Month'] = copy_df.index.month\n",
    "    \n",
    "    # Interaction Feature: Ratio of EMA_10 to EMA_20\n",
    "    for col in numerical_columns:\n",
    "        copy_df[f'{col}_EMA_Ratio'] = copy_df[f'{col}_EMA_10'] / copy_df[f'{col}_EMA_20']\n",
    "    \n",
    "    # Drop NA values caused by shifting and rolling\n",
    "    copy_df.dropna(inplace=True)\n",
    "    \n",
    "    return copy_df\n",
    "\n",
    "\n",
    "def quicky_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the data by converting the 'Date' column to datetime,\n",
    "    setting it as index, and dropping unnecessary columns.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Raw data.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Preprocessed data.\n",
    "    \"\"\"\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "    \n",
    "    if 'Index' in df.columns:\n",
    "        df.drop(columns=['Index'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def select_features_by_correlation(df, target_col=\"VN_Index_Close\", train_ratio=0.9, corr_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Splits the DataFrame by time (first train_ratio% of rows is 'training'),\n",
    "    calculates correlation of each feature with the target on TRAIN rows only,\n",
    "    and returns the subset of columns (target + selected features).\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Full dataset (includes the target column).\n",
    "        target_col (str): Target column name, default = \"VN_Index_Close\".\n",
    "        train_ratio (float): Proportion of data used for 'training'.\n",
    "        corr_threshold (float): Minimum absolute correlation needed to keep a feature.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A filtered DataFrame with only 'target_col' + selected features.\n",
    "    \"\"\"\n",
    "    # Sort by index if needed (assuming your index is Date or similar)\n",
    "    df = df.sort_index()\n",
    "    n_train = int(len(df) * train_ratio)\n",
    "    \n",
    "    # TRAIN portion (first 90% by default)\n",
    "    df_train = df.iloc[:n_train]\n",
    "    \n",
    "    # Identify all potential features (exclude the target itself)\n",
    "    all_features = [col for col in df.columns if col != target_col]\n",
    "    \n",
    "    # Calculate absolute correlation with the target on the training portion only\n",
    "    corr_series = df_train[all_features].corrwith(df_train[target_col]).abs()\n",
    "    \n",
    "    # Filter by threshold\n",
    "    selected_features = corr_series[corr_series >= corr_threshold].index.tolist()\n",
    "    \n",
    "    print(f\"Features with abs(corr) >= {corr_threshold}:\")\n",
    "    print(selected_features)\n",
    "    \n",
    "    # Return only target + selected features\n",
    "    return df[[target_col] + selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“‚ Load dataset\n",
    "file_path_1 = \"../ready_data/cleaned_hose_historical_data.csv\"\n",
    "df_1 = pd.read_csv(file_path_1)\n",
    "df_1 = quicky_data(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 13:47:58,800] A new study created in memory with name: no-name-7fcba30c-e329-4a39-b429-991c7659d428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter tuning with Optuna...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 13:49:16,597] Trial 0 finished with value: 0.0006023622341899681 and parameters: {'hidden_size': 128, 'num_layers': 3, 'dropout': 0.4789544148764239, 'learning_rate': 0.00035551470653832874, 'batch_size': 32}. Best is trial 0 with value: 0.0006023622341899681.\n",
      "/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.23686695254840717 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-03-27 13:49:38,021] Trial 1 finished with value: 0.00012096099342784328 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.23686695254840717, 'learning_rate': 0.0021636018321328264, 'batch_size': 32}. Best is trial 1 with value: 0.00012096099342784328.\n",
      "[I 2025-03-27 13:50:09,279] Trial 2 finished with value: 0.00037826484094694024 and parameters: {'hidden_size': 64, 'num_layers': 3, 'dropout': 0.13098637177270886, 'learning_rate': 0.0072071827349487845, 'batch_size': 64}. Best is trial 1 with value: 0.00012096099342784328.\n",
      "[I 2025-03-27 13:52:32,995] Trial 3 finished with value: 0.00043846268101788155 and parameters: {'hidden_size': 256, 'num_layers': 3, 'dropout': 0.4040687561350398, 'learning_rate': 0.0007410959635175076, 'batch_size': 32}. Best is trial 1 with value: 0.00012096099342784328.\n",
      "[I 2025-03-27 13:53:47,487] Trial 4 finished with value: 0.0005645494371719906 and parameters: {'hidden_size': 128, 'num_layers': 3, 'dropout': 0.47825820952304277, 'learning_rate': 0.0008827008178462572, 'batch_size': 32}. Best is trial 1 with value: 0.00012096099342784328.\n",
      "[I 2025-03-27 13:54:00,919] Trial 5 pruned. \n",
      "/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.19915345606396717 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-03-27 13:54:08,408] Trial 6 pruned. \n",
      "[I 2025-03-27 13:54:25,958] Trial 7 pruned. \n",
      "[I 2025-03-27 13:55:03,819] Trial 8 pruned. \n",
      "/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3164764593257839 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-03-27 13:55:28,437] Trial 9 finished with value: 0.0001415356730660733 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.3164764593257839, 'learning_rate': 0.002247702907542733, 'batch_size': 32}. Best is trial 1 with value: 0.00012096099342784328.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.23686695254840717, 'learning_rate': 0.0021636018321328264, 'batch_size': 32}\n",
      "Final Training Loss during tuning: 0.00012096099342784328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18/20:  27%|â–ˆâ–ˆâ–‹       | 34/128 [00:00<00:00, 105.35it/s]"
     ]
    }
   ],
   "source": [
    "# ðŸ“Š Select only VN-INDEX for prediction\n",
    "data = df_1[\"VN_Index_Close\"]\n",
    "\n",
    "# ðŸš€ Train the model and get the test set\n",
    "model, X_test_tensor, scaler, y_pred = lstm_model_pipeline(data)\n",
    "\n",
    "# ðŸ”® Generate future predictions\n",
    "future_prediction(X_test_tensor, y_pred, data, scaler, model, num_days=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Select additional VN-INDEX statistics for prediction\n",
    "data = df_1\n",
    "\n",
    "# ðŸš€ Train the model and get the test set\n",
    "model, X_test_tensor, scaler, y_pred = lstm_model_pipeline(data)\n",
    "\n",
    "# ðŸ”® Generate future predictions\n",
    "future_prediction(X_test_tensor, y_pred, data, scaler, model, num_days=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Select additional VN-INDEX statistics for prediction\n",
    "data = lag_features_indicators(df_1[['VN_Index_Close']], ['VN_Index_Close'])\n",
    "\n",
    "# ðŸš€ Train the model and get the test set\n",
    "model, X_test_tensor, scaler, y_pred = lstm_model_pipeline(data)\n",
    "\n",
    "# ðŸ”® Generate future predictions\n",
    "future_prediction(X_test_tensor, y_pred, data, scaler, model, num_days=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Select additional VN-INDEX statistics for prediction\n",
    "data = lag_features_indicators(df_1, ['VN_Index_Close'])\n",
    "\n",
    "# ðŸš€ Train the model and get the test set\n",
    "model, X_test_tensor, scaler, y_pred = lstm_model_pipeline(data)\n",
    "\n",
    "# ðŸ”® Generate future predictions\n",
    "future_prediction(X_test_tensor, y_pred, data, scaler, model, num_days=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Select additional VN-INDEX statistics for prediction\n",
    "data = lag_features_indicators(df_1, df_1.columns)\n",
    "\n",
    "# ðŸš€ Train the model and get the test set\n",
    "model, X_test_tensor, scaler, y_pred = lstm_model_pipeline(data)\n",
    "\n",
    "# ðŸ”® Generate future predictions\n",
    "future_prediction(X_test_tensor, y_pred, data, scaler, model, num_days=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“‚ Load dataset\n",
    "file_path_2 = \"../ready_data/vn_index_external_data.csv\"\n",
    "df_2 = pd.read_csv(file_path_2)\n",
    "df_2 = quicky_data(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_2\n",
    "\n",
    "# ðŸš€ Train the model and get the test set\n",
    "model, X_test_tensor, scaler, y_pred = lstm_model_pipeline(data)\n",
    "\n",
    "# ðŸ”® Generate future predictions\n",
    "future_prediction(X_test_tensor, y_pred, data, scaler, model, num_days=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = lag_features_indicators(df_2, ['VN_Index_Close'])\n",
    "\n",
    "# ðŸš€ Train the model and get the test set\n",
    "model, X_test_tensor, scaler, y_pred = lstm_model_pipeline(data)\n",
    "\n",
    "# ðŸ”® Generate future predictions\n",
    "future_prediction(X_test_tensor, y_pred, data, scaler, model, num_days=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = lag_features_indicators(df_2, df_2.columns)\n",
    "\n",
    "# ðŸš€ Train the model and get the test set\n",
    "model, X_test_tensor, scaler, y_pred = lstm_model_pipeline(data)\n",
    "\n",
    "# ðŸ”® Generate future predictions\n",
    "future_prediction(X_test_tensor, y_pred, data, scaler, model, num_days=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“‚ Load dataset\n",
    "file_path_3 = \"../ready_data/merged_data.csv\"\n",
    "df_3 = pd.read_csv(file_path_3)\n",
    "df_3 = quicky_data(df_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_3\n",
    "\n",
    "# ðŸš€ Train the model and get the test set\n",
    "model, X_test_tensor, scaler, y_pred = lstm_model_pipeline(data)\n",
    "\n",
    "# ðŸ”® Generate future predictions\n",
    "future_prediction(X_test_tensor, y_pred, data, scaler, model, num_days=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = lag_features_indicators(df_3, ['VN_Index_Close'])\n",
    "\n",
    "# ðŸš€ Train the model and get the test set\n",
    "model, X_test_tensor, scaler, y_pred = lstm_model_pipeline(data)\n",
    "\n",
    "# ðŸ”® Generate future predictions\n",
    "future_prediction(X_test_tensor, y_pred, data, scaler, model, num_days=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = lag_features_indicators(df_3, df_3.columns)\n",
    "\n",
    "# ðŸš€ Train the model and get the test set\n",
    "model, X_test_tensor, scaler, y_pred = lstm_model_pipeline(data)\n",
    "\n",
    "# ðŸ”® Generate future predictions\n",
    "future_prediction(X_test_tensor, y_pred, data, scaler, model, num_days=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = lag_features_indicators(df_3, df_1.columns)\n",
    "\n",
    "# ðŸš€ Train the model and get the test set\n",
    "model, X_test_tensor, scaler, y_pred = lstm_model_pipeline(data)\n",
    "\n",
    "# ðŸ”® Generate future predictions\n",
    "future_prediction(X_test_tensor, y_pred, data, scaler, model, num_days=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = lag_features_indicators(df_3, df_2.columns)\n",
    "\n",
    "# ðŸš€ Train the model and get the test set\n",
    "model, X_test_tensor, scaler, y_pred = lstm_model_pipeline(data)\n",
    "\n",
    "# ðŸ”® Generate future predictions\n",
    "future_prediction(X_test_tensor, y_pred, data, scaler, model, num_days=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = lag_features_indicators(df_3, df_3.columns)\n",
    "\n",
    "data = select_features_by_correlation(data)\n",
    "\n",
    "# ðŸš€ Train the model and get the test set\n",
    "model, X_test_tensor, scaler, y_pred = lstm_model_pipeline(data, tuning=True)\n",
    "\n",
    "# ðŸ”® Generate future predictions\n",
    "future_prediction(X_test_tensor, y_pred, data, scaler, model, num_days=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
