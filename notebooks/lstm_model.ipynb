{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import optuna\n",
    "\n",
    "# âœ… Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" MODEL STUFF \"\"\"\n",
    "# Define LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        return self.fc(lstm_out[:, -1, :])  # Last time step output\n",
    "\n",
    "# Helper: Create sequences for LSTM\n",
    "def create_sequences(data, dates, seq_len=30):\n",
    "    X, y, y_dates = [], [], []\n",
    "    data_array = data  # Already a NumPy array\n",
    "    for i in range(len(data_array) - seq_len):\n",
    "        X.append(data_array[i:i+seq_len])\n",
    "        y.append(data_array[i+seq_len, 0])\n",
    "        y_dates.append(dates[i+seq_len])\n",
    "    return np.array(X), np.array(y), np.array(y_dates)\n",
    "\n",
    "# Helper: Inverse scale predictions for VN-INDEX (first column)\n",
    "def inverse_scale_predictions(predictions, scaler):\n",
    "    num_features = scaler.min_.shape[0]\n",
    "    dummy = np.zeros((predictions.shape[0], num_features))\n",
    "    dummy[:, 0] = predictions.flatten()\n",
    "    return scaler.inverse_transform(dummy)[:, 0]\n",
    "\n",
    "# Training and evaluation function (used for hyperparameter tuning)\n",
    "def train_evaluate_model(params, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, epochs=20):\n",
    "    model = LSTMModel(\n",
    "        input_size=X_train_tensor.shape[2],\n",
    "        hidden_size=params['hidden_size'],\n",
    "        num_layers=params['num_layers'],\n",
    "        dropout=params['dropout']\n",
    "    ).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "    criterion = nn.MSELoss()\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(batch_X), batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            val_loss += criterion(model(batch_X), batch_y).item()\n",
    "    return val_loss / len(test_loader)\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor):\n",
    "    params = {\n",
    "        'hidden_size': trial.suggest_categorical('hidden_size', [64, 128, 256]),\n",
    "        'num_layers': trial.suggest_int('num_layers', 1, 3),\n",
    "        'dropout': trial.suggest_float('dropout', 0.1, 0.5),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    }\n",
    "    return train_evaluate_model(params, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Main function: training, tuning, evaluation, and future prediction\n",
    "def lstm_model_pipeline(data, seq_len=30, tuning=False, best_params={'hidden_size': 128, 'num_layers': 2,'dropout': 0.3, \n",
    "    'learning_rate': 1e-4, 'batch_size': 32}):\n",
    "\n",
    "    if isinstance(data, pd.Series):\n",
    "        data = data.to_frame()\n",
    "\n",
    "    # Normalize data: fit scaler on first 90% of data, then transform all\n",
    "    train_size = int(0.9 * len(data))\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(data[:train_size])\n",
    "    data_scaled = scaler.transform(data)\n",
    "    \n",
    "    # Create sequences and split into train/test based on original data indices\n",
    "    X, y, y_dates = create_sequences(data_scaled, data.index, seq_len)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "    y_dates_train, y_dates_test = y_dates[:train_size], y_dates[train_size:]\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "    \n",
    "    if tuning:\n",
    "        # Hyperparameter tuning with Optuna (fewer trials for speed)\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(lambda trial: objective(trial, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor),\n",
    "                    n_trials=10)\n",
    "        best_params = study.best_params\n",
    "        print(\"Best Hyperparameters:\", best_params)\n",
    "    \n",
    "    # Train final model using best hyperparameters\n",
    "    final_model = LSTMModel(\n",
    "        input_size=X_train_tensor.shape[2],\n",
    "        hidden_size=best_params['hidden_size'],\n",
    "        num_layers=best_params['num_layers'],\n",
    "        dropout=best_params['dropout']\n",
    "    ).to(device)\n",
    "    optimizer = optim.Adam(final_model.parameters(), lr=best_params['learning_rate'])\n",
    "    criterion = nn.MSELoss()\n",
    "    batch_size = best_params['batch_size']\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    epochs = 50\n",
    "    train_losses, val_losses = [], []\n",
    "    for epoch in range(epochs):\n",
    "        final_model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(final_model(batch_X), batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "        \n",
    "        final_model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in test_loader:\n",
    "                val_loss += criterion(final_model(batch_X), batch_y).item()\n",
    "        val_losses.append(val_loss / len(test_loader))\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label=\"Train Loss\", marker='o')\n",
    "    plt.plot(val_losses, label=\"Val Loss\", marker='s')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate final model\n",
    "    final_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_tensor = final_model(X_test_tensor).cpu().numpy()\n",
    "        y_true_tensor = y_test_tensor.cpu().numpy()\n",
    "    \n",
    "    y_pred = inverse_scale_predictions(y_pred_tensor, scaler)\n",
    "    y_true = inverse_scale_predictions(y_true_tensor, scaler)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"RMSE: {rmse:.4f}, MAE: {mae:.4f}, RÂ²: {r2:.4f}\")\n",
    "    \n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(y_dates_test, y_true, label=\"Actual VN-INDEX\", marker='o', color=\"blue\")\n",
    "    plt.plot(y_dates_test, y_pred, label=\"Predicted VN-INDEX\", marker='s', linestyle=\"dashed\", color=\"red\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"VN-INDEX\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    plt.title(\"LSTM Predictions vs. Actual VN-INDEX\")\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate percentage change between successive values for both actual and predicted series\n",
    "    actual_pct_change = np.diff(y_true.flatten()) / y_true.flatten()[:-1] * 100\n",
    "    pred_pct_change = np.diff(y_pred.flatten()) / y_pred.flatten()[:-1] * 100\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(actual_pct_change, label=\"Actual % Change\", marker='s', color=\"blue\")\n",
    "    plt.plot(pred_pct_change, label=\"Predicted % Change\", marker='s', color=\"red\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Percentage Change (%)\")\n",
    "    plt.title(\"Percentage Change Comparison: Actual vs. Predicted\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "    # Print a table of results (first 10 rows)\n",
    "    results_df = pd.DataFrame({\n",
    "        \"Date\": y_dates_test,\n",
    "        \"Actual VN-INDEX\": y_true,\n",
    "        \"Predicted VN-INDEX\": y_pred\n",
    "    })\n",
    "    print(\"Predicted vs. Actual VN-INDEX (Test Set):\")\n",
    "    print(results_df)\n",
    "    \n",
    "    return final_model, X_test_tensor, scaler, y_pred\n",
    "\n",
    "# Future Prediction Function\n",
    "def future_prediction(X_test, y_pred, data, scaler, model, num_days=30):\n",
    "\n",
    "    if isinstance(data, pd.Series):\n",
    "        data = data.to_frame()\n",
    "\n",
    "    model.eval()\n",
    "    input_seq = X_test[-1].cpu().numpy()\n",
    "    future_preds = []\n",
    "    for _ in range(num_days):\n",
    "        input_tensor = torch.tensor(input_seq, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(input_tensor).cpu().numpy()[0, 0]\n",
    "        future_preds.append(pred)\n",
    "        input_seq = np.roll(input_seq, -1, axis=0)\n",
    "        input_seq[-1, 0] = pred  # update the VN-INDEX feature\n",
    "    future_preds = inverse_scale_predictions(np.array(future_preds).reshape(-1,1), scaler)\n",
    "    \n",
    "    last_date = data.index[-1]\n",
    "    future_dates = []\n",
    "    while len(future_dates) < num_days:\n",
    "        last_date += pd.Timedelta(days=1)\n",
    "        if last_date.weekday() < 5:\n",
    "            future_dates.append(last_date)\n",
    "    \n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(future_dates, future_preds, marker='o', linestyle=\"dashed\", color=\"red\", label=\"Predicted VN-INDEX\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"VN-INDEX\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    plt.title(f\"Predicted VN-INDEX for Next {num_days} Trading Days\")\n",
    "    plt.show()\n",
    "\n",
    "    # Overlay historical data and future predictions\n",
    "    historical_dates = data.index[-100:]\n",
    "    historical_values = data.iloc[-100:, 0].values  # assuming VN-INDEX is in the first column\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(historical_dates, historical_values, label=\"Historical VN-INDEX\", color=\"blue\")\n",
    "    plt.plot(historical_dates, y_pred[-100:], label=\"Test Predictions\", color=\"red\")\n",
    "    plt.plot(future_dates, future_preds, color=\"green\", label=\"Future Predicted VN-INDEX\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"VN-INDEX\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    plt.title(\"Historical VN-INDEX with Future Predictions\")\n",
    "    plt.show()\n",
    "    \n",
    "    future_df = pd.DataFrame({\"Date\": future_dates, \"Predicted VN-INDEX\": future_preds})\n",
    "    print(future_df)\n",
    "\n",
    "# Example Usage:\n",
    "# Assuming `data` is a NumPy array of your features and `df` is your DataFrame with dates as index.\n",
    "# final_model, X_test_tensor, scaler, y_dates_test = lstm_model_pipeline(data, df)\n",
    "# future_prediction(X_test_tensor, df, scaler, final_model, num_days=30)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" DATA FEATURES \"\"\"\n",
    "def compute_RSI(series, window=14):\n",
    "        delta = series.diff(1)\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window).mean()\n",
    "        RS = gain / loss\n",
    "        return 100 - (100 / (1 + RS))\n",
    "\n",
    "def lag_features_indicators(df, numerical_columns):\n",
    "    copy_df = df.copy()\n",
    "    # ðŸ“Œ Apply Lagged Features to Every Column\n",
    "    lag_days = [1, 2, 3, 5, 10]  # Lags of 1, 2, 3, 5, and 10 days\n",
    "    for col in numerical_columns:\n",
    "        for lag in lag_days:\n",
    "            copy_df[f'{col}_Lag{lag}'] = copy_df[col].shift(lag)\n",
    "\n",
    "    # ðŸ“Œ Apply Simple Moving Averages (SMA) and Exponential Moving Averages (EMA) to Every Column\n",
    "    for col in numerical_columns:\n",
    "        copy_df[f'{col}_SMA_10'] = copy_df[col].rolling(window=10).mean()\n",
    "        copy_df[f'{col}_SMA_20'] = copy_df[col].rolling(window=20).mean()\n",
    "        copy_df[f'{col}_EMA_10'] = copy_df[col].ewm(span=10, adjust=False).mean()\n",
    "        copy_df[f'{col}_EMA_20'] = copy_df[col].ewm(span=20, adjust=False).mean()\n",
    "\n",
    "    # ðŸ“Œ Apply Relative Strength Index (RSI) to Every Column\n",
    "    for col in numerical_columns:\n",
    "        copy_df[f'{col}_RSI_14'] = compute_RSI(copy_df[col])\n",
    "\n",
    "    # ðŸ“Œ Apply Moving Average Convergence Divergence (MACD) to Every Column\n",
    "    for col in numerical_columns:\n",
    "        copy_df[f'{col}_EMA_12'] = copy_df[col].ewm(span=12, adjust=False).mean()\n",
    "        copy_df[f'{col}_EMA_26'] = copy_df[col].ewm(span=26, adjust=False).mean()\n",
    "        copy_df[f'{col}_MACD'] = copy_df[f'{col}_EMA_12'] - copy_df[f'{col}_EMA_26']\n",
    "\n",
    "    # ðŸ“Œ Drop NA values caused by shifting and rolling\n",
    "    copy_df.dropna(inplace=True)\n",
    "\n",
    "    return copy_df\n",
    "\n",
    "def quicky_data(df):\n",
    "    # ðŸ•’ Convert 'Date' to datetime and set as index\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "\n",
    "    # ðŸ“Œ Drop unnecessary columns\n",
    "    if 'Index' in df.columns:\n",
    "        df.drop(columns=['Index'], inplace=True)\n",
    "    \n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ðŸ“‚ Load dataset\n",
    "file_path_1 = \"../ready_data/cleaned_hose_historical_data.csv\"\n",
    "df_1 = pd.read_csv(file_path_1)\n",
    "df_1 = quicky_data(df_1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ðŸ“Š Select only VN-INDEX for prediction\n",
    "data = df_1[\"VN_Index_Close\"]\n",
    "\n",
    "# ðŸš€ Train the model and get the test set\n",
    "model, X_test_tensor, scaler, y_pred = lstm_model_pipeline(data)\n",
    "\n",
    "# ðŸ”® Generate future predictions\n",
    "future_prediction(X_test_tensor, y_pred, data, scaler, model, num_days=30)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ðŸ“Š Select additional VN-INDEX statistics for prediction\n",
    "data = df_1\n",
    "\n",
    "# ðŸš€ Train the model and get the test set\n",
    "model, X_test_tensor, scaler, y_pred = lstm_model_pipeline(data)\n",
    "\n",
    "# ðŸ”® Generate future predictions\n",
    "future_prediction(X_test_tensor, y_pred, data, scaler, model, num_days=30)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ðŸ“Š Select additional VN-INDEX statistics for prediction\n",
    "data = lag_features_indicators(df_1, ['VN_Index_Close'])\n",
    "\n",
    "# ðŸš€ Train the model and get the test set\n",
    "model, X_test_tensor, scaler, y_pred = lstm_model_pipeline(data)\n",
    "\n",
    "# ðŸ”® Generate future predictions\n",
    "future_prediction(X_test_tensor, y_pred, data, scaler, model, num_days=30)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ðŸ“Š Select additional VN-INDEX statistics for prediction\n",
    "data = lag_features_indicators(df_1, df_1.columns)\n",
    "\n",
    "# ðŸš€ Train the model and get the test set\n",
    "model, X_test_tensor, scaler, y_pred = lstm_model_pipeline(data)\n",
    "\n",
    "# ðŸ”® Generate future predictions\n",
    "future_prediction(X_test_tensor, y_pred, data, scaler, model, num_days=30)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ðŸ“‚ Load dataset\n",
    "file_path_2 = \"../ready_data/vn_index_external_data.csv\"\n",
    "df_2 = pd.read_csv(file_path_2)\n",
    "df_2 = quicky_data(df_2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = df_2\n",
    "\n",
    "# ðŸš€ Train the model and get the test set\n",
    "model, X_test_tensor, scaler, y_pred = lstm_model_pipeline(data)\n",
    "\n",
    "# ðŸ”® Generate future predictions\n",
    "future_prediction(X_test_tensor, y_pred, data, scaler, model, num_days=30)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = lag_features_indicators(df_2, ['VN_Index_Close'])\n",
    "\n",
    "# ðŸš€ Train the model and get the test set\n",
    "model, X_test_tensor, scaler, y_pred = lstm_model_pipeline(data)\n",
    "\n",
    "# ðŸ”® Generate future predictions\n",
    "future_prediction(X_test_tensor, y_pred, data, scaler, model, num_days=30)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = lag_features_indicators(df_2, df_2.columns)\n",
    "\n",
    "# ðŸš€ Train the model and get the test set\n",
    "model, X_test_tensor, scaler, y_pred = lstm_model_pipeline(data)\n",
    "\n",
    "# ðŸ”® Generate future predictions\n",
    "future_prediction(X_test_tensor, y_pred, data, scaler, model, num_days=30)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ðŸ“‚ Load dataset\n",
    "file_path_3 = \"../ready_data/merged_data.csv\"\n",
    "df_3 = pd.read_csv(file_path_3)\n",
    "df_3 = quicky_data(df_3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = df_3\n",
    "\n",
    "# ðŸš€ Train the model and get the test set\n",
    "model, X_test_tensor, scaler, y_pred = lstm_model_pipeline(data)\n",
    "\n",
    "# ðŸ”® Generate future predictions\n",
    "future_prediction(X_test_tensor, y_pred, data, scaler, model, num_days=30)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = lag_features_indicators(df_3, ['VN_Index_Close'])\n",
    "\n",
    "# ðŸš€ Train the model and get the test set\n",
    "model, X_test_tensor, scaler, y_pred = lstm_model_pipeline(data)\n",
    "\n",
    "# ðŸ”® Generate future predictions\n",
    "future_prediction(X_test_tensor, y_pred, data, scaler, model, num_days=30)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = lag_features_indicators(df_3, df_3.columns)\n",
    "\n",
    "# ðŸš€ Train the model and get the test set\n",
    "model, X_test_tensor, scaler, y_pred = lstm_model_pipeline(data)\n",
    "\n",
    "# ðŸ”® Generate future predictions\n",
    "future_prediction(X_test_tensor, y_pred, data, scaler, model, num_days=30)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = lag_features_indicators(df_3, df_1.columns)\n",
    "\n",
    "# ðŸš€ Train the model and get the test set\n",
    "model, X_test_tensor, scaler, y_pred = lstm_model_pipeline(data)\n",
    "\n",
    "# ðŸ”® Generate future predictions\n",
    "future_prediction(X_test_tensor, y_pred, data, scaler, model, num_days=30)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = lag_features_indicators(df_3, df_2.columns)\n",
    "\n",
    "# ðŸš€ Train the model and get the test set\n",
    "model, X_test_tensor, scaler, y_pred = lstm_model_pipeline(data)\n",
    "\n",
    "# ðŸ”® Generate future predictions\n",
    "future_prediction(X_test_tensor, y_pred, data, scaler, model, num_days=30)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.11.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.11.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}